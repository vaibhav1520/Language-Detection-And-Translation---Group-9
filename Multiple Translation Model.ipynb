{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJykEqnULEDy"
      },
      "source": [
        "\r\n",
        "import string\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import collections\r\n",
        "import helper\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "JrjcVZkgxt6y",
        "outputId": "a46eaed3-3316-493c-bb9a-94ed9ee471de"
      },
      "source": [
        "eng_df=pd.read_csv(\"/transltn_english.txt\",\"utf-8\",header=None,names=[\"English\"])\r\n",
        "fre_df=pd.read_csv(\"/translt_french.txt\",\"utf-8\",header=None,names=[\"French\"])\r\n",
        "ger_df=pd.read_csv(\"/transltn_german.txt\",\"utf-8\",header=None,names=[\"German\"])\r\n",
        "span_df=pd.read_csv(\"/transltn_spanish.txt\",\"utf-8\",header=None,names=[\"Spanish\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8d31401b4ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meng_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/transltn_english.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"English\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfre_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/translt_french.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"French\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mger_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/transltn_german.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"German\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspan_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/transltn_spanish.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spanish\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 )\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2387\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m         )\n\u001b[1;32m   2391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/transltn_english.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxVsFC3yoaR"
      },
      "source": [
        "translate_table= dict((ord(char),None)for char in string.punctuation)\r\n",
        "data_eng=[]\r\n",
        "lang_eng=[]\r\n",
        "data_ger=[]\r\n",
        "lang_ger=[]\r\n",
        "data_fre=[]\r\n",
        "lang_fre=[]\r\n",
        "data_span=[]\r\n",
        "lang_span=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7IPHRdY0Apx"
      },
      "source": [
        "j=1\r\n",
        "for i,line in eng_df.iterrows():\r\n",
        "  line=line['English']\r\n",
        "  if len(line) !=0:\r\n",
        "    line=line.lower()\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "    line=re.sub(r\"\\d+\",\"\",line)\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "      j=j-1\r\n",
        "    line=line.translate(translate_table)\r\n",
        "    data_eng.append(line)\r\n",
        "    lang_eng.append(\"English\")\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGeSXCkb0o3A"
      },
      "source": [
        "j=1\r\n",
        "for i,line in fre_df.iterrows():\r\n",
        "  line=line['French']\r\n",
        "  if len(line) !=0:\r\n",
        "    line=line.lower()\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "    line=re.sub(r\"\\d+\",\"\",line)\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "      j=j-1\r\n",
        "    line=line.translate(translate_table)\r\n",
        "    data_fre.append(line)\r\n",
        "    lang_fre.append(\"French\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7h4L6Y8Itwu"
      },
      "source": [
        "j=1\r\n",
        "for i,line in ger_df.iterrows():\r\n",
        "  line=line['German']\r\n",
        "  if len(line) !=0:\r\n",
        "    line=line.lower()\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "    line=re.sub(r\"\\d+\",\"\",line)\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "      j=j-1\r\n",
        "    line=line.translate(translate_table)\r\n",
        "    data_ger.append(line)\r\n",
        "    lang_ger.append(\"German\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTY2WxPCIyIE"
      },
      "source": [
        "j=1\r\n",
        "for i,line in span_df.iterrows():\r\n",
        "  line=line['Spanish']\r\n",
        "  if len(line) !=0:\r\n",
        "    line=line.lower()\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "    line=re.sub(r\"\\d+\",\"\",line)\r\n",
        "    if j>0:\r\n",
        "      print(line)\r\n",
        "      j=j-1\r\n",
        "    line=line.translate(translate_table)\r\n",
        "    data_span.append(line)\r\n",
        "    lang_span.append(\"Spanish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF8Ay2n70wjP"
      },
      "source": [
        "df=pd.DataFrame({\"Text\":data_eng + data_ger + data_fre + data_span,\r\n",
        "                 \"language\":lang_eng + lang_ger + lang_fre + lang_span})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPp3EerN00xP"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in data_eng for word in sentence.split()])\r\n",
        "french_words_counter = collections.Counter([word for sentence in data_fre for word in sentence.split()])\r\n",
        "print('{} English words.'.format(len([word for sentence in data_eng for word in sentence.split()])))\r\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\r\n",
        "print('10 Most common words in the English dataset:')\r\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\r\n",
        "print()\r\n",
        "print('{} French words.'.format(len([word for sentence in data_fre for word in sentence.split()])))\r\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\r\n",
        "print('10 Most common words in the French dataset:')\r\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECmj7HF424hZ"
      },
      "source": [
        "def tokenize(x):\r\n",
        "    \"\"\"\r\n",
        "    Tokenize x\r\n",
        "    :param x: List of sentences/strings to be tokenized\r\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\r\n",
        "    \"\"\"\r\n",
        "    # TODO: Implement\r\n",
        "    tokenizer = Tokenizer()\r\n",
        "    tokenizer.fit_on_texts(x)\r\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9O5uWg_29qn"
      },
      "source": [
        "def pad(x, length=None):\r\n",
        "    \"\"\"\r\n",
        "    Pad x\r\n",
        "    :param x: List of sequences.\r\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\r\n",
        "    :return: Padded numpy array of sequences\r\n",
        "    \"\"\"\r\n",
        "    # TODO: Implement\r\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjXgniTH3BBP"
      },
      "source": [
        "def preprocess(x, y):\r\n",
        "    \"\"\"\r\n",
        "    Preprocess x and y\r\n",
        "    :param x: Feature List of sentences\r\n",
        "    :param y: Label List of sentences\r\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\r\n",
        "    \"\"\"\r\n",
        "    preprocess_x, x_tk = tokenize(x)\r\n",
        "    preprocess_y, y_tk = tokenize(y)\r\n",
        "\r\n",
        "    preprocess_x = pad(preprocess_x)\r\n",
        "    preprocess_y = pad(preprocess_y)\r\n",
        "\r\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\r\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\r\n",
        "\r\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\r\n",
        "\r\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\r\n",
        "    preprocess(data_eng, data_fre)\r\n",
        "    \r\n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\r\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\r\n",
        "english_vocab_size = len(english_tokenizer.word_index)\r\n",
        "french_vocab_size = len(french_tokenizer.word_index)\r\n",
        "\r\n",
        "print('Data Preprocessed')\r\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\r\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\r\n",
        "print(\"English vocabulary size:\", english_vocab_size)\r\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZfVCVJ63arH"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    Turn logits from a neural network into text using the tokenizer\r\n",
        "    :param logits: Logits from a neural network\r\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\r\n",
        "    :return: String that represents the text of the logits\r\n",
        "    \"\"\"\r\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\r\n",
        "    index_to_words[0] = '<PAD>'\r\n",
        "\r\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqrTYU7THSP6"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\r\n",
        "    \"\"\"\r\n",
        "    Build and train a RNN model using word embedding on x and y\r\n",
        "    :param input_shape: Tuple of input shape\r\n",
        "    :param output_sequence_length: Length of output sequence\r\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\r\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\r\n",
        "    :return: Keras model built, but not trained\r\n",
        "    \"\"\"\r\n",
        "    # TODO: Implement\r\n",
        "\r\n",
        "    # Hyperparameters\r\n",
        "    learning_rate = 0.01\r\n",
        "    \r\n",
        "    # TODO: Build the layers\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\r\n",
        "    model.add(GRU(256, return_sequences=True))    \r\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\r\n",
        "    model.add(Dropout(0.5))\r\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \r\n",
        "\r\n",
        "    # Compile model\r\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\r\n",
        "                  optimizer=Adam(learning_rate),\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F_l_FdHHV9H"
      },
      "source": [
        "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\r\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\r\n",
        "tmp_x=tmp_x[:4190]\r\n",
        "print(len(tmp_x))\r\n",
        "print(len(preproc_french_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePAV1u3dHZZe"
      },
      "source": [
        "embed_rnn_model = embed_model(\r\n",
        "    tmp_x.shape,\r\n",
        "    preproc_french_sentences.shape[1],\r\n",
        "    len(english_tokenizer.word_index)+1,\r\n",
        "    len(french_tokenizer.word_index)+1)\r\n",
        "\r\n",
        "embed_rnn_model.summary()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvgNQVYqIexG"
      },
      "source": [
        "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\r\n",
        "\r\n",
        "# TODO: Print prediction(s)\r\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\r\n",
        "print(\"Prediction:\")\r\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\r\n",
        "\r\n",
        "print(\"\\nCorrect Translation:\")\r\n",
        "print(data_fre[:1])\r\n",
        "\r\n",
        "print(\"\\nOriginal text:\")\r\n",
        "print(data_eng[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZW7cci0LK1_"
      },
      "source": [
        "embed_rnn_model.save('translation.h5')\r\n",
        "!ls\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "melBGTnqOsXG"
      },
      "source": [
        "shape_m1 = embed_rnn_model.to_json() # shape of model\r\n",
        "with open('embed_rnn_model.json','w') as myFile:\r\n",
        "  myFile.write(shape_m1)\r\n",
        "\r\n",
        "embed_rnn_model.save_weights('embed_rnn_modelweights.h5')\r\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY5yL4AWIQOc"
      },
      "source": [
        "german_words_counter = collections.Counter([word for sentence in data_ger for word in sentence.split()])\r\n",
        "spanish_words_counter = collections.Counter([word for sentence in data_span for word in sentence.split()])\r\n",
        "print('{} germa words.'.format(len([word for sentence in data_ger for word in sentence.split()])))\r\n",
        "print('{} unique English words.'.format(len(german_words_counter)))\r\n",
        "print('10 Most common words in the English dataset:')\r\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\r\n",
        "print()\r\n",
        "print('{} French words.'.format(len([word for sentence in data_span for word in sentence.split()])))\r\n",
        "print('{} unique French words.'.format(len(spanish_words_counter)))\r\n",
        "print('10 Most common words in the French dataset:')\r\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XADVlYjWk2li"
      },
      "source": [
        "preproc_english_sentences, preproc_german_sentences, english_tokenizer, german_tokenizer =\\\r\n",
        "    preprocess(data_eng, data_ger)\r\n",
        "    \r\n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\r\n",
        "max_german_sequence_length = preproc_german_sentences.shape[1]\r\n",
        "english_vocab_size = len(english_tokenizer.word_index)\r\n",
        "german_vocab_size = len(french_tokenizer.word_index)\r\n",
        "\r\n",
        "print('Data Preprocessed')\r\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\r\n",
        "print(\"Max German sentence length:\", max_german_sequence_length)\r\n",
        "print(\"English vocabulary size:\", english_vocab_size)\r\n",
        "print(\"german vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hrKX5U3lZxL"
      },
      "source": [
        "preproc_english_sentences, preproc_spanish_sentences, english_tokenizer, spanish_tokenizer =\\\r\n",
        "    preprocess(data_eng, data_span)\r\n",
        "    \r\n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\r\n",
        "max_spanish_sequence_length = preproc_german_sentences.shape[1]\r\n",
        "english_vocab_size = len(english_tokenizer.word_index)\r\n",
        "spanish_vocab_size = len(french_tokenizer.word_index)\r\n",
        "\r\n",
        "print('Data Preprocessed')\r\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\r\n",
        "print(\"Max spanish sentence length:\", max_spanish_sequence_length)\r\n",
        "print(\"English vocabulary size:\", english_vocab_size)\r\n",
        "print(\"spanish vocabulary size:\", spanish_vocab_size)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "009sZJrRmkKi"
      },
      "source": [
        "tmp_y = pad(preproc_english_sentences, preproc_german_sentences.shape[1])\r\n",
        "tmp_y = tmp_y.reshape((-1, preproc_german_sentences.shape[-2]))\r\n",
        "print(len(tmp_y))\r\n",
        "print(len(preproc_german_sentences))\r\n",
        "\r\n",
        "tmp_z = pad(preproc_english_sentences, preproc_spanish_sentences.shape[1])\r\n",
        "tmp_z = tmp_z.reshape((-1, preproc_spanish_sentences.shape[-2]))\r\n",
        "print(len(tmp_z))\r\n",
        "print(len(preproc_spanish_sentences))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xuZ3g84sJZW"
      },
      "source": [
        "embed_rnn_model_span = embed_model(\r\n",
        "    tmp_y.shape,\r\n",
        "    preproc_spanish_sentences.shape[1],\r\n",
        "    len(english_tokenizer.word_index)+1,\r\n",
        "    len(spanish_tokenizer.word_index)+1)\r\n",
        "\r\n",
        "embed_rnn_model_span.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hn3np8IsZTf"
      },
      "source": [
        "\r\n",
        "embed_rnn_model_span.fit(tmp_z, preproc_spanish_sentences, batch_size=1024, epochs=20, validation_split=0.2)\r\n",
        "# TODO: Print prediction(s)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA17UXgks7RA"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print(logits_to_text(embed_rnn_model_span.predict(tmp_x[:1])[0], spanish_tokenizer))\r\n",
        "print(\"Prediction:\")\r\n",
        "print(logits_to_text(embed_rnn_model_span.predict(tmp_x[:1])[0], spanish_tokenizer))\r\n",
        "\r\n",
        "print(\"\\nCorrect Translation:\")\r\n",
        "print(data_span[:1])\r\n",
        "\r\n",
        "print(\"\\nOriginal text:\")\r\n",
        "print(data_eng[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmRgLBZhvau4"
      },
      "source": [
        "embed_rnn_model_ger = embed_model(\r\n",
        "    tmp_y.shape,\r\n",
        "    preproc_german_sentences.shape[1],\r\n",
        "    len(english_tokenizer.word_index)+1,\r\n",
        "    len(german_tokenizer.word_index)+1)\r\n",
        "\r\n",
        "embed_rnn_model_ger.summary()\r\n",
        "embed_rnn_model_ger.fit(tmp_y, preproc_german_sentences, batch_size=1024, epochs=20, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3usVJPaKvwzT"
      },
      "source": [
        "print(logits_to_text(embed_rnn_model_ger.predict(tmp_x[:1])[0], german_tokenizer))\r\n",
        "print(\"Prediction:\")\r\n",
        "print(logits_to_text(embed_rnn_model_ger.predict(tmp_x[:1])[0], german_tokenizer))\r\n",
        "\r\n",
        "print(\"\\nCorrect Translation:\")\r\n",
        "print(data_ger[:1])\r\n",
        "\r\n",
        "print(\"\\nOriginal text:\")\r\n",
        "print(data_eng[:1])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnzHCQCE5mwQ"
      },
      "source": [
        "embed_rnn_model_ger.save('german.h5')\r\n",
        "embed_rnn_model_span.save('spanish.h5')\r\n",
        "\r\n",
        "shape_m2 =embed_rnn_model_ger.to_json() # shape of model\r\n",
        "with open('embed_rnn_model_ger.json','w') as myFile:\r\n",
        "  myFile.write(shape_m2)\r\n",
        "\r\n",
        "embed_rnn_model_ger.save_weights('german_weights.h5') \r\n",
        "\r\n",
        "shape_m3 = embed_rnn_model_span.to_json() # shape of model\r\n",
        "with open('embed_rnn_model_span.json','w') as myFile:\r\n",
        "  myFile.write(shape_m3)\r\n",
        "\r\n",
        "embed_rnn_model_span.save_weights('spanish_weights.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgowY2Sk6ZmD"
      },
      "source": [
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n006A47Z6cNW"
      },
      "source": [
        "X=tmp_y\r\n",
        "Y=preproc_german_sentences\r\n",
        "\r\n",
        "json_file = open('embed_rnn_model_ger.json', 'r')\r\n",
        "loaded_model_json = json_file.read()\r\n",
        "json_file.close()\r\n",
        "loaded_model = model_from_json(loaded_model_json)\r\n",
        "# load weights into new model\r\n",
        "loaded_model.load_weights(\"german.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        " \r\n",
        "# evaluate loaded model on test data\r\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G40DU-Uryj5I"
      },
      "source": [
        "tmp_y[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQViqu0r7GMb"
      },
      "source": [
        "X=tmp_z\r\n",
        "Y=preproc_spanish_sentences\r\n",
        "\r\n",
        "json_file = open('embed_rnn_model_span.json', 'r')\r\n",
        "loaded_model_json = json_file.read()\r\n",
        "json_file.close()\r\n",
        "loaded_model = model_from_json(loaded_model_json)\r\n",
        "# load weights into new model\r\n",
        "loaded_model.load_weights(\"spanish.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        " \r\n",
        "# evaluate loaded model on test data\r\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "score = loaded_model.evaluate(X, Y, verbose=0)\r\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvWAu1AE8vM"
      },
      "source": [
        "c='katze'\r\n",
        "k=0\r\n",
        "l=1\r\n",
        "flag=0\r\n",
        "for j in data_ger:\r\n",
        "  words=[]\r\n",
        "  for word in j.split():\r\n",
        "    if word != '.':\r\n",
        "      word = word.replace('.','')\r\n",
        "      words.append(word)\r\n",
        "  for i in words:\r\n",
        "    k=k+1\r\n",
        "    if i==c:\r\n",
        "      if flag==0:\r\n",
        "        print(i)\r\n",
        "        K=k\r\n",
        "        L=l\r\n",
        "        print(\"word number=\",k)\r\n",
        "        print(\"sentence number=\",l)\r\n",
        "        flag=1\r\n",
        "  l=l+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3rjdDT-btdA"
      },
      "source": [
        "print(L,K)\r\n",
        "for u in data_eng:\r\n",
        "  L=L-1 \r\n",
        "  if L==0:\r\n",
        "    print(\"L:\",L)\r\n",
        "    for v in u.split():\r\n",
        "      words1=[]\r\n",
        "      if v != '.':\r\n",
        "        v = v.replace('.','')\r\n",
        "        words1.append(v)\r\n",
        "        print(\"words1:\",words1)\r\n",
        "      for n in words1:\r\n",
        "        K=K-1\r\n",
        "        if K==0:\r\n",
        "          b=n\r\n",
        "          print(b)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}